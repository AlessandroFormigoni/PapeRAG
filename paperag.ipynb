{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ace01a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_classic import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "import gradio as gr\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d45831",
   "metadata": {},
   "source": [
    "<p>Import llm tools</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4a82d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI (\n",
    "api_key = \"ollama\" ,\n",
    "model = \"qwen3:4b\" ,\n",
    "base_url = \"http://localhost:11434/v1\" ,\n",
    "temperature = 0\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65465d1",
   "metadata": {},
   "source": [
    "<p>Import document(s) and embed them in a vector store</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27984c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/aless/Documents/Rebedea2023.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # chunk size (characters)\n",
    "    chunk_overlap=125,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cec9b",
   "metadata": {},
   "source": [
    "<p>Create state graph</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53bb65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    chat_history: List[dict]\n",
    "    conversation_summary: str  \n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert at summarizing conversations. \n",
    "Create a concise summary of the conversation that captures the key topics discussed, \n",
    "important questions asked, and main points from the answers.\n",
    "Keep technical terms and specific references (like theorem names, definitions, etc.).\"\"\"),\n",
    "    (\"human\", \"\"\"Current Summary (if any):\n",
    "{current_summary}\n",
    "\n",
    "New Exchange:\n",
    "User: {question}\n",
    "Assistant: {answer}\n",
    "\n",
    "Updated Summary:\"\"\")\n",
    "])\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    current_summary = state.get(\"conversation_summary\", \"\")\n",
    "    \n",
    "    if not current_summary:\n",
    "        current_summary = \"No previous conversation.\"\n",
    "    \n",
    "    messages = summary_prompt.invoke({\n",
    "        \"current_summary\": current_summary,\n",
    "        \"question\": state[\"question\"],\n",
    "        \"answer\": state[\"answer\"]\n",
    "    })\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"conversation_summary\": response.content}\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    pattern = r'\\b(definition|theorem|proposition|lemma|corollary)\\s+(\\d+)\\b'\n",
    "    match = re.search(pattern, question, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        semantic_docs = vector_store.similarity_search(question, k=3)\n",
    "        all_candidates = vector_store.similarity_search(question, k=50)\n",
    "        \n",
    "        keyword_docs = []\n",
    "        search_term = match.group().lower()\n",
    "        \n",
    "        for doc in all_candidates:\n",
    "            if search_term in doc.page_content.lower():\n",
    "                keyword_docs.append(doc)\n",
    "                if len(keyword_docs) >= 5:\n",
    "                    break\n",
    "        \n",
    "        combined_docs = keyword_docs + [d for d in semantic_docs if d not in keyword_docs]\n",
    "        retrieved_docs = combined_docs[:5]\n",
    "        \n",
    "    else:\n",
    "        retrieved_docs = vector_store.similarity_search(question, k=5)\n",
    "    \n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    conversation_context = state.get(\"conversation_summary\", \"No previous conversation.\")\n",
    "    \n",
    "    messages = prompt.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": docs_content,\n",
    "        \"summary\": conversation_context\n",
    "    })\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the retrieved context and conversation summary to provide accurate answers.\"\"\"),\n",
    "    (\"human\", \"\"\"Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Conversation Summary:\n",
    "{summary}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "])\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate, summarize_conversation])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930352e",
   "metadata": {},
   "source": [
    "<p>Chat with PapeRAG using Gradio</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_fn(message: str, history: List[dict]):\n",
    "    if not hasattr(chat_fn, 'summary'):\n",
    "        chat_fn.summary = \"\"\n",
    "    \n",
    "    chat_state = {\n",
    "        \"question\": message,\n",
    "        \"conversation_summary\": chat_fn.summary,\n",
    "        \"chat_history\": []\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(chat_state)\n",
    "    answer = result[\"answer\"]\n",
    "    \n",
    "    chat_fn.summary = result.get(\"conversation_summary\", \"\")\n",
    "    \n",
    "    return {\"role\": \"assistant\", \"content\": answer}\n",
    "\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"PapeRAG\",\n",
    "    description=\"Chat with a Retrieval-Augmented model for academic paper analysis.\",\n",
    "    type=\"messages\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
