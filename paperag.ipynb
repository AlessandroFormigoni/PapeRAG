{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ace01a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_classic import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d45831",
   "metadata": {},
   "source": [
    "<p>Import llm tools</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a82d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI (\n",
    "api_key = \"ollama\" ,\n",
    "model = \"qwen3\" ,\n",
    "base_url = \"http://localhost:11434/v1\" ,\n",
    "temperature = 0\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65465d1",
   "metadata": {},
   "source": [
    "<p>Import document(s) and embed them in a vector store</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27984c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 134 sub-documents.\n",
      "['a5b8dfbe-5890-44b8-ae3a-c5943016ccc5', 'f3414950-aeb3-4cfe-87ea-fd5a94843cbe', '892a99fb-f35d-4fc7-b685-d1c2b11b69f7']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"GFNFoundations.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,  # chunk size (characters)\n",
    "    chunk_overlap=250,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cec9b",
   "metadata": {},
   "source": [
    "<p>Create state graph</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53bb65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    chat_history: List[dict]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant for question-answering tasks. Use the following context to answer the user's question accurately and concisely.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    \n",
    "    history_messages = []\n",
    "    for msg in state.get(\"chat_history\", []):\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            history_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            history_messages.append(AIMessage(content=msg[\"content\"]))\n",
    "    \n",
    "    messages = prompt.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": docs_content,\n",
    "        \"chat_history\": history_messages\n",
    "    })\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930352e",
   "metadata": {},
   "source": [
    "<p>Chat with PapeRAG using Gradio</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b11cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_fn(message: str, history: List[dict]):\n",
    "    chat_state = {\n",
    "        \"question\": message,\n",
    "        \"chat_history\": history.copy()\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(chat_state)\n",
    "    answer = result[\"answer\"]\n",
    "\n",
    "    return {\"role\": \"assistant\", \"content\": answer}\n",
    "\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"PapeRAG\",\n",
    "    description=\"Chat with a Retrieval-Augmented model for academic paper analysis.\",\n",
    "    type=\"messages\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
