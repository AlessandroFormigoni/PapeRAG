{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace01a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alessandro\\miniconda3\\envs\\dt_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_classic import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "import gradio as gr\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d45831",
   "metadata": {},
   "source": [
    "<p>Import llm tools</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a82d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI (\n",
    "api_key = \"ollama\" ,\n",
    "model = \"qwen3:4b\" ,\n",
    "base_url = \"http://localhost:11434/v1\" ,\n",
    "temperature = 0\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65465d1",
   "metadata": {},
   "source": [
    "<p>Import document(s) and embed them in a vector store</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27984c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 51 sub-documents.\n",
      "['071a6535-872c-4cd1-8257-5879d3ef0c90', '04f2def7-a5d8-49b9-bdaf-979a08fe3b88', '250402d9-19f8-4f33-b103-e00190fda171']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Bengio2021.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,  # chunk size (characters)\n",
    "    chunk_overlap=250,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cec9b",
   "metadata": {},
   "source": [
    "<p>Create state graph</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bb65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    chat_history: List[dict]\n",
    "    conversation_summary: str  \n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert at summarizing conversations. \n",
    "Create a concise summary of the conversation that captures the key topics discussed, \n",
    "important questions asked, and main points from the answers.\n",
    "Keep technical terms and specific references (like theorem names, definitions, etc.).\"\"\"),\n",
    "    (\"human\", \"\"\"Current Summary (if any):\n",
    "{current_summary}\n",
    "\n",
    "New Exchange:\n",
    "User: {question}\n",
    "Assistant: {answer}\n",
    "\n",
    "Updated Summary:\"\"\")\n",
    "])\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    current_summary = state.get(\"conversation_summary\", \"\")\n",
    "    \n",
    "    if not current_summary:\n",
    "        current_summary = \"No previous conversation.\"\n",
    "    \n",
    "    messages = summary_prompt.invoke({\n",
    "        \"current_summary\": current_summary,\n",
    "        \"question\": state[\"question\"],\n",
    "        \"answer\": state[\"answer\"]\n",
    "    })\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"conversation_summary\": response.content}\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    pattern = r'\\b(definition|theorem|proposition|lemma|corollary)\\s+(\\d+)\\b'\n",
    "    match = re.search(pattern, question, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        semantic_docs = vector_store.similarity_search(question, k=3)\n",
    "        all_candidates = vector_store.similarity_search(question, k=50)\n",
    "        \n",
    "        keyword_docs = []\n",
    "        search_term = match.group().lower()\n",
    "        \n",
    "        for doc in all_candidates:\n",
    "            if search_term in doc.page_content.lower():\n",
    "                keyword_docs.append(doc)\n",
    "                if len(keyword_docs) >= 5:\n",
    "                    break\n",
    "        \n",
    "        combined_docs = keyword_docs + [d for d in semantic_docs if d not in keyword_docs]\n",
    "        retrieved_docs = combined_docs[:5]\n",
    "        \n",
    "    else:\n",
    "        retrieved_docs = vector_store.similarity_search(question, k=5)\n",
    "    \n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    conversation_context = state.get(\"conversation_summary\", \"No previous conversation.\")\n",
    "    \n",
    "    messages = prompt.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": docs_content,\n",
    "        \"summary\": conversation_context\n",
    "    })\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the retrieved context and conversation summary to provide accurate answers.\"\"\"),\n",
    "    (\"human\", \"\"\"Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Conversation Summary:\n",
    "{summary}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "])\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate, summarize_conversation])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930352e",
   "metadata": {},
   "source": [
    "<p>Chat with PapeRAG using Gradio</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the retrieved context, Proposition 3 states:\n",
      "\n",
      "When trajectories τ are sampled from an exploratory policy P with the same support as the optimal π (defined in Eq. 5) for a consistent flow F*∈F*, and assuming we have a sufficiently rich family of predictors (so that ∃θ: Fθ = F*), then:\n",
      "\n",
      "1. The global optimum θ* of the expected training loss (EP(τ)[Lθ(τ)]) will produce the correct flow model: Fθ* = F*\n",
      "\n",
      "2. The loss Lθ*(τ) will be 0 for all trajectories τ sampled from P(θ)\n",
      "\n",
      "3. If the policy πθ* satisfies the condition πθ*(a|s) = Fθ*(s,a)/∑a'∈A(s) Fθ*(s,a'), then the resulting policy distribution will be πθ*(x) = R(x)/Z\n",
      "\n",
      "This proposition demonstrates that a global optimum of the expected loss provides the correct flow model, and importantly, it shows that this method is an off-policy offline method - meaning we can use any broad-support policy to sample training trajectories and still obtain the correct flows and generative model. The text notes this approach is analogous to asynchronous dynamic programming (Sutton and Barto, 2018), which converges when every state is visited infinitely many times asymptotically.\n",
      "Updated Summary: **Updated Summary:**  \n",
      "Proposition 3 specifies conditions under which the global optimum θ* of the expected training loss (EP(τ)[Lθ(τ)]) yields correct flow modeling and policy behavior. Key conditions include:  \n",
      "- Trajectories τ sampled from an exploratory policy P with the same support as the optimal π (Eq. 5).  \n",
      "- Consistent flow F* ∈ F* and a sufficiently rich predictor family (∃θ: Fθ = F*).  \n",
      "\n",
      "**Key conclusions**:  \n",
      "1. Fθ* = F* (correct flow model).  \n",
      "2. Lθ*(τ) = 0 for all τ sampled from P(θ).  \n",
      "3. If πθ* satisfies πθ*(a|s) = Fθ*(s,a)/∑a'∈A(s) Fθ*(s,a'), then πθ*(x) = R(x)/Z.  \n",
      "\n",
      "The proposition confirms this method is an **off-policy offline approach**—any broad-support policy can sample trajectories while guaranteeing correct flow models. It is analogous to asynchronous dynamic programming (Sutton and Barto, 2018), which converges when every state is visited infinitely often asymptotically.\n",
      "Answer: The proof of Proposition 3 is as follows:\n",
      "\n",
      "**Proposition 3**: Given that there exists a consistent flow $F^* \\in \\mathcal{F}^*$ (satisfying Eq. 4) and there exists $\\theta$ such that $F_\\theta = F^*$ (sufficiently rich predictor family), and $\\theta^* = \\arg\\min_{\\theta} \\mathbb{E}_\\tau[L_\\theta(\\tau)]$ (global minimizer of expected training loss), we have:\n",
      "\n",
      "1. $F_{\\theta^*} = F^*$  \n",
      "2. $L_{\\theta^*}(\\tau) = 0$ for all $\\tau \\sim P(\\theta)$  \n",
      "3. If $\\pi_{\\theta^*}(a|s) = \\frac{F_{\\theta^*}(s,a)}{\\sum_{a'} F_{\\theta^*}(s,a')}$, then $\\pi_{\\theta^*}(x) = \\frac{R(x)}{Z}$\n",
      "\n",
      "**Proof**:\n",
      "\n",
      "1. **$F_{\\theta^*} = F^*$**:  \n",
      "   Since $\\theta^*$ is the global minimizer of the expected training loss $\\mathbb{E}_\\tau[L_\\theta(\\tau)]$, and $F_\\theta = F^*$ holds for the optimal $\\theta$, it follows that $F_{\\theta^*} = F^*$.\n",
      "\n",
      "2. **$L_{\\theta^*}(\\tau) = 0$ for all $\\tau \\sim P(\\theta)$**:  \n",
      "   By definition of $\\theta^*$, it minimizes $\\mathbb{E}_\\tau[L_\\theta(\\tau)]$. Since $L_\\theta(\\tau) \\geq 0$ for all $\\theta$ and $\\tau$, the minimizer $\\theta^*$ achieves $L_{\\theta^*}(\\tau) = 0$ for all $\\tau$ in the support of $P(\\theta)$.\n",
      "\n",
      "3. **Policy definition for terminal states**:  \n",
      "   The policy $\\pi_{\\theta^*}(x)$ is defined in terms of the flow $F_{\\theta^*}$ as $\\pi_{\\theta^*}(x) = \\frac{R(x)}{Z}$, where $R(x)$ and $Z$ are derived from the flow dynamics. This follows directly from the structure of the policy in the context of the consistent flow $F^*$.\n",
      "\n",
      "This completes the proof of Proposition 3. The key insight is that the optimal policy $\\pi_{\\theta^*}$ is fully determined by the consistent flow $F^*$, ensuring that the terminal state probabilities are well-defined and consistent with the flow dynamics.\n",
      "Updated Summary: **Updated Summary**:  \n",
      "User requested a proof of Proposition 3. The assistant provided a formal proof confirming that under the conditions of a consistent flow $F^* \\in \\mathcal{F}^*$ (Eq. 4) and a sufficiently rich predictor family ($\\exists \\theta: F_\\theta = F^*$), the global minimizer $\\theta^* = \\arg\\min_{\\theta} \\mathbb{E}_\\tau[L_\\theta(\\tau)]$ satisfies:  \n",
      "1. $F_{\\theta^*} = F^*$ (correct flow model),  \n",
      "2. $L_{\\theta^*}(\\tau) = 0$ for all $\\tau$ sampled from the exploratory policy $P$ (with support matching $\\pi$),  \n",
      "3. $\\pi_{\\theta^*}(a|s) = \\frac{F_{\\theta^*}(s,a)}{\\sum_{a'} F_{\\theta^*}(s,a')}$, yielding $\\pi_{\\theta^*}(x) = \\frac{R(x)}{Z}$.  \n",
      "\n",
      "This proof validates Proposition 3 as an **off-policy offline approach**—any broad-support policy $P$ can sample trajectories while guaranteeing correct flow models, with convergence analogous to asynchronous dynamic programming (Sutton and Barto, 2018).\n"
     ]
    }
   ],
   "source": [
    "def chat_fn(message: str, history: List[dict]):\n",
    "    if not hasattr(chat_fn, 'summary'):\n",
    "        chat_fn.summary = \"\"\n",
    "    \n",
    "    chat_state = {\n",
    "        \"question\": message,\n",
    "        \"conversation_summary\": chat_fn.summary,\n",
    "        \"chat_history\": []\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(chat_state)\n",
    "    answer = result[\"answer\"]\n",
    "    \n",
    "    chat_fn.summary = result.get(\"conversation_summary\", \"\")\n",
    "    \n",
    "    return {\"role\": \"assistant\", \"content\": answer}\n",
    "\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"PapeRAG\",\n",
    "    description=\"Chat with a Retrieval-Augmented model for academic paper analysis.\",\n",
    "    type=\"messages\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
